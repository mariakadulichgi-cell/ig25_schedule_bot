import os
import re
import csv
import io
import time
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

import requests
from dotenv import load_dotenv

from telegram import Update
from telegram.ext import (
    Application,
    CommandHandler,
    ContextTypes,
    MessageHandler,
    filters,
)

# --- Flask "–∫–æ—Å—Ç—ã–ª—å" –¥–ª—è Render Web Service (—á—Ç–æ–±—ã –±—ã–ª –æ—Ç–∫—Ä—ã—Ç –ø–æ—Ä—Ç) ---
from threading import Thread
from flask import Flask

web = Flask(__name__)

@web.get("/")
def home():
    return "ok", 200

def run_web():
    port = int(os.environ.get("PORT", "10000"))
    web.run(host="0.0.0.0", port=port)

def keep_alive():
    Thread(target=run_web, daemon=True).start()


# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
TZ = ZoneInfo(os.getenv("TZ", "Asia/Krasnoyarsk"))  # –ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫ (+07) –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_GROUP = os.getenv("GROUP_NAME", "–ò–ì25-01–ë-–û–ú").strip()

DATE_RE = re.compile(r"\b(\d{1,2})[.\-/](\d{1,2})(?:[.\-/](\d{2,4}))?\b")
TIME_RE = re.compile(r"(\d{1,2})[.:](\d{2})\s*[‚Äì\-]\s*(\d{1,2})[.:](\d{2})")

# –ö—ç—à CSV, —á—Ç–æ–±—ã –Ω–µ –¥–µ—Ä–≥–∞—Ç—å –≥—É–≥–ª –Ω–∞ –∫–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
_CACHE_TEXT = None
_CACHE_TS = 0.0
CACHE_SECONDS = 60


def norm(s: str) -> str:
    return (s or "").replace("\xa0", " ").strip()


def norm_group(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã: —Ä–∞–∑–Ω—ã–µ —Ç–∏—Ä–µ/–ø—Ä–æ–±–µ–ª—ã -> –æ–¥–∏–Ω–∞–∫–æ–≤–æ"""
    s = norm(s)
    s = s.replace("‚Äî", "-").replace("‚Äì", "-")
    s = re.sub(r"\s+", "", s)
    return s.upper()


def normalize_time(s: str) -> str:
    s = norm(s)
    m = TIME_RE.search(s)
    if not m:
        return s
    h1, m1, h2, m2 = m.groups()
    return f"{int(h1)}:{m1}‚Äì{int(h2)}:{m2}"


def parse_ddmm(text: str) -> str | None:
    m = DATE_RE.search(text or "")
    if not m:
        return None
    dd = int(m.group(1))
    mm = int(m.group(2))
    return f"{dd:02d}.{mm:02d}"


def fetch_csv_text(url: str) -> str:
    global _CACHE_TEXT, _CACHE_TS
    now = time.time()
    if _CACHE_TEXT and (now - _CACHE_TS) < CACHE_SECONDS:
        return _CACHE_TEXT

    r = requests.get(url, timeout=25)
    r.raise_for_status()
    _CACHE_TEXT = r.text
    _CACHE_TS = now
    return r.text


def find_header_and_group_cols(rows: list[list[str]], group_name: str):
    """
    –ò—â–µ–º —Å—Ç—Ä–æ–∫—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–≥–¥–µ –µ—Å—Ç—å '–î–∞—Ç–∞' –∏ '–ß–∞—Å—ã'),
    –ø–æ—Ç–æ–º –Ω–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫–∏ –≥—Ä—É–ø–ø—ã –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö.
    """
    g_need = norm_group(group_name)

    header_row_i = None
    date_col = None
    time_col = None

    # 1) —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    for i in range(min(60, len(rows))):
        row = [norm(x) for x in rows[i]]
        low = [x.lower() for x in row]
        if "–¥–∞—Ç–∞" in low and "—á–∞—Å—ã" in low:
            header_row_i = i
            date_col = low.index("–¥–∞—Ç–∞")
            time_col = low.index("—á–∞—Å—ã")
            break

    if header_row_i is None:
        raise RuntimeError("–ù–µ –Ω–∞—à–ª–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏ '–î–∞—Ç–∞' –∏ '–ß–∞—Å—ã' –≤ —Ç–∞–±–ª–∏—Ü–µ (CSV).")

    # 2) –∫–æ–ª–æ–Ω–∫–∏ –≥—Ä—É–ø–ø—ã –ø–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 12 —Å—Ç—Ä–æ–∫)
    group_cols = []
    for i in range(header_row_i, min(header_row_i + 12, len(rows))):
        row = rows[i]
        for j, cell in enumerate(row):
            if norm_group(cell) == g_need:
                group_cols.append(j)

    group_cols = sorted(set(group_cols))
    return header_row_i, date_col, time_col, group_cols


def extract_schedule_for_date(csv_text: str, group_name: str, target_ddmm: str):
    reader = csv.reader(io.StringIO(csv_text))
    rows = list(reader)

    header_i, date_col, time_col, group_cols = find_header_and_group_cols(rows, group_name)

    if not group_cols:
        # –ø–æ–ª–µ–∑–Ω—ã–π –¥–µ–±–∞–≥: –∫–∞–∫–∏–µ –≥—Ä—É–ø–ø—ã –≤–æ–æ–±—â–µ –≤–∏–¥–∏–º
        groups_found = set()
        for i in range(min(40, len(rows))):
            for cell in rows[i]:
                c = norm(cell)
                if c.startswith("–ò–ì"):
                    groups_found.add(c)
        hint = ", ".join(sorted(groups_found)) if groups_found else "–Ω–µ –Ω–∞—à–ª–∞ –Ω–∏ –æ–¥–Ω–æ–π"
        raise RuntimeError(f"–ù–µ –Ω–∞—à–ª–∞ –∫–æ–ª–æ–Ω–∫—É –≥—Ä—É–ø–ø—ã '{group_name}'. –í —Ç–∞–±–ª–∏—Ü–µ –≤–∏–∂—É –≥—Ä—É–ø–ø—ã: {hint}")

    cur_date = ""
    cur_time = ""
    items = []

    for r in rows[header_i + 1:]:
        # –∑–∞—â–∏—Ç–∞: —Ä–∞—Å—à–∏—Ä—è–µ–º —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∞—è
        need_len = max(date_col, time_col, max(group_cols)) + 1
        if len(r) < need_len:
            r = r + [""] * (need_len - len(r))

        d_raw = norm(r[date_col])
        t_raw = norm(r[time_col])

        ddmm = parse_ddmm(d_raw)
        if ddmm:
            cur_date = ddmm

        t_norm = normalize_time(t_raw)
        if TIME_RE.search(t_raw):
            cur_time = t_norm

        if cur_date != target_ddmm:
            continue

        parts = []
        for j in group_cols:
            v = norm(r[j])
            if not v:
                continue
            # —á–∏—Å—Ç–∏–º –º—É—Å–æ—Ä
            if "—Å–µ–º–µ—Å—Ç—Ä" in v.lower() or "—É—Ç–≤–µ—Ä–∂–¥–∞—é" in v.lower():
                continue
            parts.append(v)

        if not parts:
           items.append((cur_time, "–Ω–µ—Ç –ø–∞—Ä—ã"))
           continue    

        # –æ–±—ä–µ–¥–∏–Ω—è–µ–º, —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ —Å—Ç—Ä–æ–∫ –≤–Ω—É—Ç—Ä–∏ —è—á–µ–π–∫–∏
        cell_text = "\n".join(parts).strip()
        lines = [x.strip() for x in cell_text.splitlines() if x.strip()]
        uniq_lines = []
        seen = set()
        for x in lines:
            if x not in seen:
                seen.add(x)
                uniq_lines.append(x)

        items.append((cur_time, "\n".join(uniq_lines)))

    # —É–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—ã (–≤—Ä–µ–º—è+—Ç–µ–∫—Å—Ç)
    out = []
    seen = set()
    for tm, tx in items:
        key = (tm, tx)
        if key in seen:
            continue
        seen.add(key)
        out.append((tm, tx))

    return out


def _compact_spaces(s: str) -> str:
    s = (s or "").replace("\u00a0", " ").replace("\t", " ")
    s = re.sub(r"[ ]{2,}", " ", s).strip()
    return s


def _glue_pr_lines(lines: list[str]) -> list[str]:
    """
    –°–∫–ª–µ–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ "–ø—Ä", "–ª–µ–∫", "–ø—Ä / 3-17", "/ 3-17"
    —á—Ç–æ–±—ã –æ–Ω–∏ –Ω–µ —É–µ–∑–∂–∞–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ.
    """
    out: list[str] = []
    tail_tokens = {"–ø—Ä", "–ø—Ä.", "–ª–µ–∫", "–ª–µ–∫."}

    for raw in lines:
        ln = _compact_spaces(raw)
        if not ln:
            continue

        low = ln.lower()

        # 1) –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ "–ø—Ä" –∏–ª–∏ "–ª–µ–∫" (–∏–ª–∏ —Å —Ç–æ—á–∫–æ–π) ‚Äî –ø—Ä–∏–∫–ª–µ–∏–≤–∞–µ–º –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å—Ç—Ä–æ–∫–µ
        if out and low in tail_tokens:
            out[-1] = _compact_spaces(out[-1] + " " + ln)
            continue

        # 2) –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "–ø—Ä /" –∏–ª–∏ "–ø—Ä." –∏–ª–∏ "–ª–µ–∫ /" ‚Äî —Ç–æ–∂–µ –ø—Ä–∏–∫–ª–µ–∏–≤–∞–µ–º
        if out and (low.startswith("–ø—Ä /") or low.startswith("–ø—Ä/") or low.startswith("–ª–µ–∫ /") or low.startswith("–ª–µ–∫/")):
            out[-1] = _compact_spaces(out[-1] + " " + ln)
            continue

        # 3) –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "/" (—Ç–∏–ø–∞ "/ 3-17") ‚Äî –ø—Ä–∏–∫–ª–µ–∏–≤–∞–µ–º –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π
        if out and ln.startswith("/"):
            out[-1] = _compact_spaces(out[-1] + " " + ln)
            continue

        out.append(ln)

    return out


from collections import OrderedDict

def merge_items_by_time(items):
    merged = OrderedDict()

    for tm, tx in items:
        tm = (tm or "").strip()
        tx = (tx or "").strip()
        if not tm:
            continue

        # –µ—Å–ª–∏ –µ—â—ë –Ω–µ—Ç —Ç–∞–∫–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
        if tm not in merged:
            merged[tm] = tx
        else:
            # –µ—Å–ª–∏ —Ä–∞–Ω—å—à–µ –±—ã–ª–æ "–Ω–µ—Ç –ø–∞—Ä—ã", –∞ —Å–µ–π—á–∞—Å –ø—Ä–µ–¥–º–µ—Ç ‚Äî –∑–∞–º–µ–Ω—è–µ–º
            if merged[tm].lower() == "–Ω–µ—Ç –ø–∞—Ä—ã" and tx.lower() != "–Ω–µ—Ç –ø–∞—Ä—ã":
                merged[tm] = tx
            # –µ—Å–ª–∏ –æ–±–∞ –ø—Ä–µ–¥–º–µ—Ç—ã ‚Äî –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            elif tx.lower() != "–Ω–µ—Ç –ø–∞—Ä—ã" and tx not in merged[tm]:
                merged[tm] += "\n" + tx

    return [(tm, tx) for tm, tx in merged.items()]

def format_schedule(group_name: str, ddmm: str, items: list[tuple[str, str]]) -> str:
    items = merge_items_by_time(items)    
    title = f"{group_name} ‚Äî {ddmm}:"
    if not items:
        return title + "\n‚Ä¢ –ù–µ—Ç –∑–∞–Ω—è—Ç–∏–π / –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    # 1) –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –≤—Å—ë –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ 3 –±—É–ª–ª–µ—Ç–∞ –Ω–∞ –æ–¥–∏–Ω —Å–ª–æ—Ç)
    grouped: "OrderedDict[str, list[str]]" = OrderedDict()
    for tm, tx in items:
        tm = (tm or "").strip()
        tx = (tx or "").strip()
        if not tm or not tx:
            continue

        bucket = grouped.setdefault(tm, [])
        # tx –º–æ–∂–µ—Ç –±—ã—Ç—å —É–∂–µ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–º
        for line in tx.splitlines():
            line = line.strip()
            if line:
                bucket.append(line)

    if not grouped:
        return title + "\n‚Ä¢ –ù–µ—Ç –∑–∞–Ω—è—Ç–∏–π / –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    # 2) –ß–∏—Å—Ç–∏–º –ø–æ–≤—Ç–æ—Ä—ã –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –∫—Ä–∞—Å–∏–≤–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
    out_lines = [title]
    for tm, lines in grouped.items():
        # —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        uniq = []
        for ln in lines:
            key = ln.lower()
            if key in seen:
                continue
            seen.add(key)
            uniq.append(ln)

        if not uniq:
            continue

        # –ø–µ—Ä–≤—ã–π —Ä—è–¥ ‚Äî —Å –±—É–ª–ª–µ—Ç–æ–º
        out_lines.append(f"‚Ä¢ {tm} ‚Äî {uniq[0]}")
        # –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –±–µ–∑ –±—É–ª–ª–µ—Ç–∞, —Å –æ—Ç—Å—Ç—É–ø–æ–º
        for ln in uniq[1:]:
            out_lines.append(f"  {ln}")

        out_lines.append("")  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏

    # —É–±–µ—Ä—ë–º —Ö–≤–æ—Å—Ç–æ–≤—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    while out_lines and out_lines[-1] == "":
        out_lines.pop()

    return "\n".join(out_lines)


# --- Telegram handlers ---
async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –û–¢–ï–õ–¨–ö–ê ü©µ. –î–∞–≤–∞–π –ø–æ–º–æ–≥—É —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º!\n\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "/today ‚Äî —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è\n"
        "/tomorrow ‚Äî —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –∑–∞–≤—Ç—Ä–∞\n"
        "/day 30.01 ‚Äî —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –¥–∞—Ç—É (–î–î.–ú–ú)\n\n"
        "–ú–æ–∂–Ω–æ –∏ —Ç–µ–∫—Å—Ç–æ–º: 30.01 –∏–ª–∏ ¬´–¥–µ–Ω—å 30.01¬ª"
    )


async def send_schedule(update: Update, ddmm: str):
    url = os.getenv("SHEET_CSV_URL", "").strip()
    group = os.getenv("GROUP_NAME", DEFAULT_GROUP).strip()

    if not url:
        await update.message.reply_text("–ù–µ –∑–∞–¥–∞–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è SHEET_CSV_URL.")
        return

    try:
        csv_text = fetch_csv_text(url)
        items = extract_schedule_for_date(csv_text, group, ddmm)
        msg = format_schedule(group, ddmm, items)
        await update.message.reply_text(msg)
    except Exception as e:
        await update.message.reply_text(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}")


async def cmd_today(update: Update, context: ContextTypes.DEFAULT_TYPE):
    ddmm = datetime.now(TZ).strftime("%d.%m")
    await send_schedule(update, ddmm)


async def cmd_tomorrow(update: Update, context: ContextTypes.DEFAULT_TYPE):
    ddmm = (datetime.now(TZ) + timedelta(days=1)).strftime("%d.%m")
    await send_schedule(update, ddmm)


async def cmd_day(update: Update, context: ContextTypes.DEFAULT_TYPE):
    args = context.args
    if args:
        ddmm = parse_ddmm(" ".join(args))
        if not ddmm:
            await update.message.reply_text("–§–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: /day 30.01 (–î–î.–ú–ú)")
            return
    else:
        ddmm = datetime.now(TZ).strftime("%d.%m")

    await send_schedule(update, ddmm)


async def text_day(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip().lower()

    # –ª–æ–≤–∏–º "30.01" –∏–ª–∏ "–¥–µ–Ω—å 30.01"
    m = re.search(r"(\d{1,2}[.\-/]\d{1,2}(?:[.\-/]\d{2,4})?)", text)
    if not m:
        return

    ddmm = parse_ddmm(m.group(1))
    if not ddmm:
        return

    await send_schedule(update, ddmm)


def main():
    load_dotenv()

    token = os.getenv("BOT_TOKEN", "").strip()
    if not token:
        raise RuntimeError("–ù–µ—Ç BOT_TOKEN...")

    # Render Web Service: –∑–∞–ø—É—Å–∫–∞–µ–º –º–∏–Ω–∏-–≤–µ–±, —á—Ç–æ–±—ã –±—ã–ª –ø–æ—Ä—Ç
    keep_alive()

    app = Application.builder().token(token).build()

    # –ö–æ–º–∞–Ω–¥—ã
    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("today", cmd_today))
    app.add_handler(CommandHandler("tomorrow", cmd_tomorrow))
    app.add_handler(CommandHandler("day", cmd_day))

    # –¢–µ–∫—Å—Ç —Å –¥–∞—Ç–æ–π
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, text_day))

    print("Bot started / polling")
    app.run_polling()


if __name__ == "__main__":
    main()

